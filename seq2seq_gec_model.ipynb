{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LidiiaMelnyk95/FSU_Jena_scripts/blob/main/seq2seq_gec_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CqvL1bSnp2R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QUsQTmbn_DV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/replacement_spelling_deduplicated-2.csv', sep =';')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Data Augmentation***"
      ],
      "metadata": {
        "id": "5zuxKmMzT_qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4adY86ubvIZ6",
        "outputId": "ab216186-5718-4f87-ccf7-b267baf68dcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1876, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def apply_spelling_replacement(comment, replacements):\n",
        "    try:\n",
        "        for original, replacement in replacements.items():\n",
        "            comment = comment.replace(original, replacement)\n",
        "        return comment\n",
        "    except AttributeError:\n",
        "        return None\n",
        "\n",
        "replacement_trans = {'Trans', 'trans'}\n",
        "replacements_dass = {'dass': 'daß'}\n",
        "replacements_umlaute = {'ä': 'ae', 'ö': 'oe', 'ü': 'ue', 'Ä': 'Ae', 'Ö': 'Oe', 'Ü': 'Ue'}\n",
        "replacements_comma = {',': ''}\n",
        "\n",
        "# Apply spelling replacements\n",
        "for replacements in [replacements_dass, replacements_umlaute, replacements_comma]:\n",
        "    df['SPELLING'] = [apply_spelling_replacement(comment, replacements) for comment in df['Comment'][::5]]\n",
        "    df = pd.concat([df, pd.DataFrame({'SPELLING': df['SPELLING'], 'Comment': df['Comment'][::5]})], ignore_index=True)\n",
        "\n",
        "df.shape\n",
        "df.to_csv('data_augmented_df.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPUwbO5hvOCq"
      },
      "source": [
        "added 972 examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVL2v4gXwBmc"
      },
      "outputs": [],
      "source": [
        "df['encoder_input'] = df[\"SPELLING\"]\n",
        "df[\"decoder_output\"] = df[\"Comment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "INUWu1Nr0Qds",
        "outputId": "3e0eadd5-5b18-404c-f121-89239d8a8547"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           SPELLING  \\\n",
              "0      1543.0  :) Bei Partner:innenakrobatikfestivals stapeln...   \n",
              "1      2558.0   :D okay ... deswegen geht einem die Regenboge...   \n",
              "2       679.0   \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...   \n",
              "3      2290.0   \"ja schön. Diese Person kann sich ja auch dem...   \n",
              "4       822.0   \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....   \n",
              "\n",
              "                                             Comment  \\\n",
              "0   :) Bei Partner:innenakrobatikfestivals stapel...   \n",
              "1   :D okay ... deswegen geht einem die Regenboge...   \n",
              "2   \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...   \n",
              "3   \"ja schön. Diese Person kann sich ja auch dem...   \n",
              "4   \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....   \n",
              "\n",
              "                                       encoder_input  \\\n",
              "0  :) Bei Partner:innenakrobatikfestivals stapeln...   \n",
              "1   :D okay ... deswegen geht einem die Regenboge...   \n",
              "2   \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...   \n",
              "3   \"ja schön. Diese Person kann sich ja auch dem...   \n",
              "4   \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....   \n",
              "\n",
              "                                      decoder_output  \n",
              "0   :) Bei Partner:innenakrobatikfestivals stapel...  \n",
              "1   :D okay ... deswegen geht einem die Regenboge...  \n",
              "2   \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...  \n",
              "3   \"ja schön. Diese Person kann sich ja auch dem...  \n",
              "4   \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-661e2250-a106-4bf5-8767-f6bc0c559162\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>SPELLING</th>\n",
              "      <th>Comment</th>\n",
              "      <th>encoder_input</th>\n",
              "      <th>decoder_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1543.0</td>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapeln...</td>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapel...</td>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapeln...</td>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2558.0</td>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>679.0</td>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2290.0</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>822.0</td>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-661e2250-a106-4bf5-8767-f6bc0c559162')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-661e2250-a106-4bf5-8767-f6bc0c559162 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-661e2250-a106-4bf5-8767-f6bc0c559162');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HautICooEro"
      },
      "outputs": [],
      "source": [
        "df['decoder_input'] = df['decoder_output'].apply(lambda x: '<start>' + ' ' + x + ' ' +  '<end>')\n",
        "df_big = df[['encoder_input', 'decoder_input', 'decoder_output']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "2iDfQjpxvkK9",
        "outputId": "d176daa1-d61e-4c2a-9d54-b24343da17ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         encoder_input  \\\n",
              "0    :) Bei Partner:innenakrobatikfestivals stapeln...   \n",
              "1     :D okay ... deswegen geht einem die Regenboge...   \n",
              "2     \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...   \n",
              "3     \"ja schön. Diese Person kann sich ja auch dem...   \n",
              "4     \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....   \n",
              "..                                                 ...   \n",
              "308  Ich ging die ganze Zeit davon aus dass dieses ...   \n",
              "309  Meinst du die können dann theoretisch Olympia ...   \n",
              "310  Transgendersoldaten an die Front das wird der ...   \n",
              "311  „Ich bin ja nicht transfeindlich aber hat im F...   \n",
              "312  Kann mir jemand logisch erklären ob was dagege...   \n",
              "\n",
              "                                         decoder_input  \\\n",
              "0    <start>  :) Bei Partner:innenakrobatikfestival...   \n",
              "1    <start>  :D okay ... deswegen geht einem die R...   \n",
              "2    <start>  \"Das Naturgesetz\" lmaoIch hoffe du tr...   \n",
              "3    <start>  \"ja schön. Diese Person kann sich ja ...   \n",
              "4    <start>  \"Meine Seele\", \"mein Körper\" \"ich\", \"...   \n",
              "..                                                 ...   \n",
              "308  <start> Ich ging die ganze Zeit davon aus, das...   \n",
              "309  <start> Meinst du die können dann theoretisch ...   \n",
              "310  <start> Transgendersoldaten an die Front das w...   \n",
              "311  <start> „Ich bin ja nicht transfeindlich, aber...   \n",
              "312  <start> Kann mir jemand logisch erklären, ob w...   \n",
              "\n",
              "                                        decoder_output  \n",
              "0     :) Bei Partner:innenakrobatikfestivals stapel...  \n",
              "1     :D okay ... deswegen geht einem die Regenboge...  \n",
              "2     \"Das Naturgesetz\" lmaoIch hoffe du trägst kei...  \n",
              "3     \"ja schön. Diese Person kann sich ja auch dem...  \n",
              "4     \"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....  \n",
              "..                                                 ...  \n",
              "308  Ich ging die ganze Zeit davon aus, dass dieses...  \n",
              "309  Meinst du die können dann theoretisch Olympia ...  \n",
              "310  Transgendersoldaten an die Front das wird der ...  \n",
              "311  „Ich bin ja nicht transfeindlich, aber hat im ...  \n",
              "312  Kann mir jemand logisch erklären, ob was dageg...  \n",
              "\n",
              "[1876 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25b09e17-d478-4c45-8681-abb84d6fa64a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>encoder_input</th>\n",
              "      <th>decoder_input</th>\n",
              "      <th>decoder_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapeln...</td>\n",
              "      <td>&lt;start&gt;  :) Bei Partner:innenakrobatikfestival...</td>\n",
              "      <td>:) Bei Partner:innenakrobatikfestivals stapel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "      <td>&lt;start&gt;  :D okay ... deswegen geht einem die R...</td>\n",
              "      <td>:D okay ... deswegen geht einem die Regenboge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "      <td>&lt;start&gt;  \"Das Naturgesetz\" lmaoIch hoffe du tr...</td>\n",
              "      <td>\"Das Naturgesetz\" lmaoIch hoffe du trägst kei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "      <td>&lt;start&gt;  \"ja schön. Diese Person kann sich ja ...</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "      <td>&lt;start&gt;  \"Meine Seele\", \"mein Körper\" \"ich\", \"...</td>\n",
              "      <td>\"Meine Seele\", \"mein Körper\" \"ich\", \"ich\", .....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>Ich ging die ganze Zeit davon aus dass dieses ...</td>\n",
              "      <td>&lt;start&gt; Ich ging die ganze Zeit davon aus, das...</td>\n",
              "      <td>Ich ging die ganze Zeit davon aus, dass dieses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>Meinst du die können dann theoretisch Olympia ...</td>\n",
              "      <td>&lt;start&gt; Meinst du die können dann theoretisch ...</td>\n",
              "      <td>Meinst du die können dann theoretisch Olympia ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>Transgendersoldaten an die Front das wird der ...</td>\n",
              "      <td>&lt;start&gt; Transgendersoldaten an die Front das w...</td>\n",
              "      <td>Transgendersoldaten an die Front das wird der ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>„Ich bin ja nicht transfeindlich aber hat im F...</td>\n",
              "      <td>&lt;start&gt; „Ich bin ja nicht transfeindlich, aber...</td>\n",
              "      <td>„Ich bin ja nicht transfeindlich, aber hat im ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>Kann mir jemand logisch erklären ob was dagege...</td>\n",
              "      <td>&lt;start&gt; Kann mir jemand logisch erklären, ob w...</td>\n",
              "      <td>Kann mir jemand logisch erklären, ob was dageg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1876 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25b09e17-d478-4c45-8681-abb84d6fa64a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-25b09e17-d478-4c45-8681-abb84d6fa64a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-25b09e17-d478-4c45-8681-abb84d6fa64a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df_big"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AqaZ1cvZzN"
      },
      "outputs": [],
      "source": [
        "df_big = df_big.sample(frac = 1).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training Preparation***"
      ],
      "metadata": {
        "id": "BAHn35bdUG-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzANIY6ByePV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "train, test = train_test_split(df_big, test_size=0.10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwOIREbfz7Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed927c54-dafb-4714-8b0b-2eb845b1a3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9232\n",
            "8280\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim.downloader as api\n",
        "\n",
        "def preprocess_text_with_punctuation(text):\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    return text.translate(translator)\n",
        "\n",
        "def fit_tokenizer(texts, filters=''):\n",
        "    tokenizer = Tokenizer(filters=filters, lower=False)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def create_embedding_matrix(tokenizer, embeddings_index):\n",
        "    embedding_dim = len(embeddings_index.get(embeddings_index.index_to_key[0]))\n",
        "    words_and_punctuation = list(tokenizer.word_index.keys()) + list(string.punctuation)\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in words_and_punctuation:\n",
        "            # Handle hyphens in word embeddings\n",
        "            word = word.replace('-', '_')\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "# Load the German GloVe model\n",
        "model = api.load(\"glove-twitter-25\")\n",
        "\n",
        "# Preprocess the text to include punctuation as separate tokens\n",
        "train['encoder_input'] = train['encoder_input'].apply(preprocess_text_with_punctuation)\n",
        "train['decoder_input'] = train['decoder_input'].apply(preprocess_text_with_punctuation)\n",
        "\n",
        "# Fit the tokenizers on the preprocessed data\n",
        "token_perturbation = fit_tokenizer(list(train['encoder_input'].values) + list(string.punctuation), filters='')\n",
        "token_correct = fit_tokenizer(list(train['decoder_input'].values) + list(string.punctuation), filters='')\n",
        "\n",
        "# Save tokenizers using pickle\n",
        "with open(\"token_perturbation.pkl\", \"wb\") as f:\n",
        "    pickle.dump(token_perturbation, f)\n",
        "\n",
        "with open(\"token_correct.pkl\", \"wb\") as f:\n",
        "    pickle.dump(token_correct, f)\n",
        "\n",
        "# Create embedding matrices\n",
        "embedding_matrix_perturbation = create_embedding_matrix(token_perturbation, model)\n",
        "embedding_matrix_correct = create_embedding_matrix(token_correct, model)\n",
        "\n",
        "# Print the shapes of the embedding matrices\n",
        "print(f\"Embedding Matrix Shape for perturbation: {embedding_matrix_perturbation.shape}\")\n",
        "print(f\"Embedding Matrix Shape for correct: {embedding_matrix_correct.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG1buEHI0x1i"
      },
      "outputs": [],
      "source": [
        "pickle.dump(embedding_matrix, open('embedding_matrix.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT7FYubT1DAp"
      },
      "outputs": [],
      "source": [
        "pickle.dump(embedding_matrix_per, open('embedding_matrix_per.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Architecture***"
      ],
      "metadata": {
        "id": "ihysw--7ULJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjV839bM1HtI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.enc_units= enc_units\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h=0\n",
        "        self.lstm_state_c=0\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n",
        "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "    def call(self, input_sentances, training=True):\n",
        "        input_embedd                        = self.embedding(input_sentances)\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
        "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
        "    def get_states(self):\n",
        "        return self.lstm_state_h,self.lstm_state_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfSHlm-C1LcG"
      },
      "outputs": [],
      "source": [
        "encoder_vanilla = pickle.dump(Encoder, open('encodv.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJyI64R51QyW"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = 25\n",
        "        self.dec_units = dec_units\n",
        "        self.input_length = input_length\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # we are using embedding_matrix weights and not training the embedding layer\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n",
        "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "    def call(self, target_sentances, state_h, state_c):\n",
        "        target_embedd           = self.embedding(target_sentances)\n",
        "        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
        "        return lstm_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t9Hj0eU1UMU"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, token_perturbation, token_correct, max_len):\n",
        "        self.encoder_inps = data['encoder_input'].values\n",
        "        self.decoder_inps = data['decoder_input'].values\n",
        "        self.decoder_outs = data['decoder_output'].values\n",
        "        self.token_correct = token_correct\n",
        "        self.token_perturbation = token_perturbation\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.token_perturbation.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.token_correct.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.token_correct.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int64', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int64', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int64', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "\n",
        "        # we are creating data like ([perturbation, correct_inp], correct_out) these are already converted into seq\n",
        "\n",
        "        return [batch[0],batch[1]],batch[2]\n",
        "\n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK17qqDe1ai1"
      },
      "outputs": [],
      "source": [
        "class vanilla_model(Model):\n",
        "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size=v_perturbation, embedding_dim=25, input_length=encoder_inputs_length, enc_units=512)\n",
        "        self.decoder = Decoder(vocab_size=v_correct, embedding_dim=25, input_length=decoder_inputs_length, dec_units=512)\n",
        "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
        "        self.dropout = tf.keras.layers.Dropout(0.01)\n",
        "        self.normalization =  tf.keras.layers.LayerNormalization(epsilon=0.005)\n",
        "\n",
        "\n",
        "    def call(self, data):\n",
        "        input,output = data[0], data[1]\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
        "        decoder_output                       = self.decoder(tf.cast(output[:8869], tf.int64), encoder_h, encoder_c)\n",
        "        decoder_output = self.normalization(decoder_output)\n",
        "        decoder_output = self.dropout(decoder_output)\n",
        "        output                               = self.dense(decoder_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model evaluation metrics***"
      ],
      "metadata": {
        "id": "cDH10La9UQLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KymOyFD7Pe4f",
        "outputId": "e49fd328-db2f-4858-d655-5153c0569a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHZdHU5q1l2A"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "metrics_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: print(compute_metrics(val_data, model.predict(val_data))))\n"
      ],
      "metadata": {
        "id": "4QPmWM8nPfWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Training***"
      ],
      "metadata": {
        "id": "QgaFf_0rUZ7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tgYQJcE1deF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f675ed11-39bf-4fc3-81fc-1f64b537e328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 100) (100, 100) (100, 100)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train, token_perturbation, token_correct,100 )\n",
        "test_dataset = Dataset(test, token_perturbation, token_correct, 100)\n",
        "\n",
        "train_dataloader = Dataloader(train_dataset, batch_size=100)\n",
        "test_dataloader = Dataloader(test_dataset, batch_size=100)\n",
        "\n",
        "\n",
        "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwt4562s1o2J"
      },
      "outputs": [],
      "source": [
        "vanilla = vanilla_model(encoder_inputs_length=512, decoder_inputs_length=512, output_vocab_size=v_perturbation)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0007)\n",
        "vanilla.compile(optimizer= optimizer, loss= loss_function, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bvdQLaZJTve"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "logfile = \"/content//Project_2/logs/vanilla/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "#logfile = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\logs\\\\vanilla\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tfboard = tf.keras.callbacks.TensorBoard(log_dir=logfile, histogram_freq=1, write_graph=True)\n",
        "\n",
        "chkfile = \"/content/Project_2/wts/vanilla/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "#chkfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\vanilla\\\\weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "chkpt = tf.keras.callbacks.ModelCheckpoint(chkfile, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n",
        "\n",
        "stp = tf.keras.callbacks.EarlyStopping(patience=7, monitor='val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO1K5TZeMtCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba158c31-c959-40f3-ef90-6b7717b2830b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "16/16 [==============================] - 17s 407ms/step - loss: 2.5402 - accuracy: 0.2440\n",
            "Epoch 2/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 2.1466 - accuracy: 0.2624\n",
            "Epoch 3/50\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 2.0594 - accuracy: 0.2662\n",
            "Epoch 4/50\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 1.9942 - accuracy: 0.2714\n",
            "Epoch 5/50\n",
            "16/16 [==============================] - 3s 168ms/step - loss: 2.0221 - accuracy: 0.2711\n",
            "Epoch 6/50\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 1.9871 - accuracy: 0.2717\n",
            "Epoch 7/50\n",
            "16/16 [==============================] - 3s 166ms/step - loss: 1.9837 - accuracy: 0.2705\n",
            "Epoch 8/50\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 2.1423 - accuracy: 0.2622\n",
            "Epoch 9/50\n",
            "16/16 [==============================] - 3s 165ms/step - loss: 2.0718 - accuracy: 0.2647\n",
            "Epoch 10/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.9824 - accuracy: 0.2681\n",
            "Epoch 11/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.8993 - accuracy: 0.2729\n",
            "Epoch 12/50\n",
            "16/16 [==============================] - 3s 165ms/step - loss: 1.8469 - accuracy: 0.2754\n",
            "Epoch 13/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.8032 - accuracy: 0.2765\n",
            "Epoch 14/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.7645 - accuracy: 0.2771\n",
            "Epoch 15/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 1.7222 - accuracy: 0.2787\n",
            "Epoch 16/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.6875 - accuracy: 0.2811\n",
            "Epoch 17/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.6411 - accuracy: 0.2864\n",
            "Epoch 18/50\n",
            "16/16 [==============================] - 3s 160ms/step - loss: 1.5939 - accuracy: 0.2963\n",
            "Epoch 19/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.5424 - accuracy: 0.3153\n",
            "Epoch 20/50\n",
            "16/16 [==============================] - 3s 160ms/step - loss: 1.4908 - accuracy: 0.3340\n",
            "Epoch 21/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 1.4283 - accuracy: 0.3573\n",
            "Epoch 22/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 1.3756 - accuracy: 0.3758\n",
            "Epoch 23/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 1.3241 - accuracy: 0.3940\n",
            "Epoch 24/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 1.2686 - accuracy: 0.4088\n",
            "Epoch 25/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.2204 - accuracy: 0.4241\n",
            "Epoch 26/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 1.1766 - accuracy: 0.4396\n",
            "Epoch 27/50\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 1.1390 - accuracy: 0.4552\n",
            "Epoch 28/50\n",
            "16/16 [==============================] - 3s 165ms/step - loss: 1.0937 - accuracy: 0.4753\n",
            "Epoch 29/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 1.0524 - accuracy: 0.4930\n",
            "Epoch 30/50\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 1.0113 - accuracy: 0.5124\n",
            "Epoch 31/50\n",
            "16/16 [==============================] - 3s 170ms/step - loss: 0.9713 - accuracy: 0.5292\n",
            "Epoch 32/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.9346 - accuracy: 0.5452\n",
            "Epoch 33/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.8996 - accuracy: 0.5610\n",
            "Epoch 34/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 0.8739 - accuracy: 0.5720\n",
            "Epoch 35/50\n",
            "16/16 [==============================] - 3s 166ms/step - loss: 0.8443 - accuracy: 0.5876\n",
            "Epoch 36/50\n",
            "16/16 [==============================] - 3s 171ms/step - loss: 0.8120 - accuracy: 0.6028\n",
            "Epoch 37/50\n",
            "16/16 [==============================] - 3s 166ms/step - loss: 0.7822 - accuracy: 0.6185\n",
            "Epoch 38/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 0.7594 - accuracy: 0.6314\n",
            "Epoch 39/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 0.7405 - accuracy: 0.6401\n",
            "Epoch 40/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.7095 - accuracy: 0.6552\n",
            "Epoch 41/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 0.6777 - accuracy: 0.6726\n",
            "Epoch 42/50\n",
            "16/16 [==============================] - 3s 160ms/step - loss: 0.6462 - accuracy: 0.6910\n",
            "Epoch 43/50\n",
            "16/16 [==============================] - 3s 165ms/step - loss: 0.6184 - accuracy: 0.7056\n",
            "Epoch 44/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 0.5963 - accuracy: 0.7163\n",
            "Epoch 45/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 0.5708 - accuracy: 0.7283\n",
            "Epoch 46/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 0.5523 - accuracy: 0.7388\n",
            "Epoch 47/50\n",
            "16/16 [==============================] - 3s 161ms/step - loss: 0.5314 - accuracy: 0.7470\n",
            "Epoch 48/50\n",
            "16/16 [==============================] - 3s 164ms/step - loss: 0.5117 - accuracy: 0.7549\n",
            "Epoch 49/50\n",
            "16/16 [==============================] - 3s 163ms/step - loss: 0.4925 - accuracy: 0.7639\n",
            "Epoch 50/50\n",
            "16/16 [==============================] - 3s 162ms/step - loss: 0.4770 - accuracy: 0.7700\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f16b5cd4df0>"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "train_steps=train.shape[0]//16\n",
        "vanilla.fit(train_dataloader,  epochs =50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "from rouge import Rouge\n",
        "\n",
        "# Define a custom callback to compute Rouge score during training\n",
        "class RougeScoreCallback(Callback):\n",
        "    def __init__(self, encoder_input_val, decoder_input_val, target_val):\n",
        "        self.encoder_input_val = encoder_input_val\n",
        "        self.decoder_input_val = decoder_input_val\n",
        "        self.target_val = target_val\n",
        "        self.rouge = Rouge()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        translations = []\n",
        "        targets = []\n",
        "        for i in range(len(self.encoder_input_val)):\n",
        "            enc_input = self.encoder_input_val[i]\n",
        "            dec_input = self.decoder_input_val[i]\n",
        "            target = self.target_val[i]\n",
        "            translation = inference(enc_input, dec_input)\n",
        "            translations.append(translation)\n",
        "            targets.append(target)\n",
        "        scores = self.rouge.get_scores(translations, targets, avg=True)\n",
        "        logs = logs or {}\n",
        "        logs['rouge-1'] = scores['rouge-1']['f']\n",
        "        logs['rouge-2'] = scores['rouge-2']['f']\n",
        "        logs['rouge-l'] = scores['rouge-l']['f']\n",
        "        print(f\"Rouge-1: {logs['rouge-1']:.4f} - Rouge-2: {logs['rouge-2']:.4f} - Rouge-L: {logs['rouge-l']:.4f}\")\n",
        "        return logs\n",
        "\n",
        "# Instantiate the custom callback\n",
        "rouge_score_callback = RougeScoreCallback(encoder_input_val, decoder_input_val, target_val)\n",
        "\n",
        "# Train the model with the custom callback\n",
        "vanilla.fit(train_dataloader, steps_per_epoch=train_steps, epochs=200, validation_data=(test_dataloader), callbacks=[rouge_score_callback])\n"
      ],
      "metadata": {
        "id": "cy-1GcoPTXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhFFN1QSnc_9"
      },
      "outputs": [],
      "source": [
        "vanilla.build([512,32,256])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5BbcqVgn-2V"
      },
      "outputs": [],
      "source": [
        "output_test = vanilla.predict(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBKrd_vYqBPp"
      },
      "outputs": [],
      "source": [
        "some_list =list(token_correct.word_index.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Inference***"
      ],
      "metadata": {
        "id": "C8qmbciRUfVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbgbq9ehqjah"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def inference(enc_inp, dec_inp):\n",
        "    translation = \"\"\n",
        "\n",
        "    # Convert encoder input to sequences and pad\n",
        "    e_input = pad_sequences(token_perturbation.texts_to_sequences([enc_inp]), maxlen=16, padding='post')\n",
        "\n",
        "    # Get encoder output and states\n",
        "    e_output, e_hidden, e_cell = vanilla.layers[0](e_input)\n",
        "\n",
        "    # Convert decoder input to sequences and pad\n",
        "    d_input = pad_sequences(token_correct.texts_to_sequences([dec_inp]), maxlen=16, padding='post')\n",
        "\n",
        "    # Generate prediction\n",
        "    prediction = vanilla.layers[2](vanilla.layers[1](d_input, e_hidden, e_cell))\n",
        "\n",
        "    # Retrieve words from indices in the prediction\n",
        "    for word_index in tf.argmax(prediction, axis=2).numpy()[0]:\n",
        "        try:\n",
        "            word = token_correct.index_word[word_index]\n",
        "            translation += word + \" \"\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    return translation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "Be3jVYKSJp1m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "bbfa3360-f6b2-4d26-c015-d2cb62cc61d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      index                                      encoder_input  \\\n",
              "541     118  Ist es wenn man sich die Definition von transs...   \n",
              "432     238  Ich ging die ganze Zeit davon aus ,  dass dies...   \n",
              "792     661  Mulm ,  damit hätte ich nicht gerechnet .  Vie...   \n",
              "637     327   der rapide anstieg ist allerdings ein ziemlic...   \n",
              "1687    162  War ja klar dass diese Scheiße gut findet .  ....   \n",
              "\n",
              "                                          decoder_input  \\\n",
              "541    < start >  Ist es wenn man sich die Definitio...   \n",
              "432    < start >  Ich ging die ganze Zeit davon aus ...   \n",
              "792    < start >  Mulm ,  damit hätte ich nicht gere...   \n",
              "637    < start >  Der rapide Anstieg ist allerdings ...   \n",
              "1687   < start >  War ja klar dass diese Scheiße gut...   \n",
              "\n",
              "                                         decoder_output  \n",
              "541   Ist es wenn man sich die Definition von Transs...  \n",
              "432   Ich ging die ganze Zeit davon aus, dass dieses...  \n",
              "792   Mulm, damit hätte ich nicht gerechnet. Vielen ...  \n",
              "637   Der rapide Anstieg ist allerdings ein ziemlich...  \n",
              "1687  War ja klar dass diese Scheiße gut findet..gut...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebcad13f-426d-462a-a9e6-4d152f2fa190\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>encoder_input</th>\n",
              "      <th>decoder_input</th>\n",
              "      <th>decoder_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>118</td>\n",
              "      <td>Ist es wenn man sich die Definition von transs...</td>\n",
              "      <td>&lt; start &gt;  Ist es wenn man sich die Definitio...</td>\n",
              "      <td>Ist es wenn man sich die Definition von Transs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>238</td>\n",
              "      <td>Ich ging die ganze Zeit davon aus ,  dass dies...</td>\n",
              "      <td>&lt; start &gt;  Ich ging die ganze Zeit davon aus ...</td>\n",
              "      <td>Ich ging die ganze Zeit davon aus, dass dieses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>661</td>\n",
              "      <td>Mulm ,  damit hätte ich nicht gerechnet .  Vie...</td>\n",
              "      <td>&lt; start &gt;  Mulm ,  damit hätte ich nicht gere...</td>\n",
              "      <td>Mulm, damit hätte ich nicht gerechnet. Vielen ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>327</td>\n",
              "      <td>der rapide anstieg ist allerdings ein ziemlic...</td>\n",
              "      <td>&lt; start &gt;  Der rapide Anstieg ist allerdings ...</td>\n",
              "      <td>Der rapide Anstieg ist allerdings ein ziemlich...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687</th>\n",
              "      <td>162</td>\n",
              "      <td>War ja klar dass diese Scheiße gut findet .  ....</td>\n",
              "      <td>&lt; start &gt;  War ja klar dass diese Scheiße gut...</td>\n",
              "      <td>War ja klar dass diese Scheiße gut findet..gut...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebcad13f-426d-462a-a9e6-4d152f2fa190')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ebcad13f-426d-462a-a9e6-4d152f2fa190 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ebcad13f-426d-462a-a9e6-4d152f2fa190');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCZDCyGUsYeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8218cf0-2358-4012-8fc1-d966c6ca3a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trotzdem die Psyche von denen lässt aber sich sich sich sich die auch bewusst die nicht es ist eben zu ist ist ist \n",
            "Der Der kontra Einfach finde jeder Weil für der kann ich kann dann es es es ich welche die die das Fälle stimmt stimmt mal wenn wenn wenn tatsächlich tatsächlich tatsächlich ich ich ich Aufwand lieber bin bin bin bin bin mir mir mir mir mir mir mir mir mir geben mir und und und und und und und und \n",
            "Godfather die dir ganz sprechen ganz Viele beachten ist ist in dieser Zeit dieser dieser dieser vor seinen ihre \n",
            "Der die die die die die \n",
            "kontra der dran Perspektive mit mit mit nachdem ich ich ich nicht nicht nicht nicht durch ziemlich von Hormonbehandlungen lange lange lange lange lange wie weshalb was leider gesagt was zum zum und zum der und sich sich Am sich sich sich der und Wenn und ganz Angst Angst und ich ich ich ich sehr neuen es dem dem dem kann Namen mir mir mir mir neuen heute dem dem nur nur gut Diskussion der der der der von von Diskussion Ende Ende sein dass wohl wohl auch nicht auch auch dann ich ich Diskussion so ich so Ich was was was was nur auf nur nur jetzt bei jetzt so so paar paar andere du Charakter die negativen negativen negativen Studie jedoch gehen Gründe Gründe von Psychotherapeuten muss muss aber aber oder zu aber aber gehen vor Und ich man man ist man man den man Und Und auch auch auch auch Und auch auch und ich man auch auch auch gut auch Und viele zu zu vor vor vor gesehen aber vor zu idf gesehen es es ich gesehen zu zu Person Person zu WIRKLICH und und ist zu zu Detransition und sollte vor sollte Person sollte sollte Person Person so Person Person \n",
            "Der ich ich \n",
            "Größe Seite total Weil bitte In Thema In Thema total Mann Mann machen zu zu zu zu zu eher die zu ja gut in sehr sehr sehr diagnostiziert immer immer immer ist sie immer völlig es sie das wie das das um oder oder \n",
            "Der Ich und und chirurgisch und es Wenn der der der wie wie formulieren da da da da meist meist der der der mal Bei und Ich Ich zu zu zu zu wirklich wirklich wirklich wirklich diesen sehr ich ich ein diesen Am beispielsweise durch macht die die und man man man Mutter Mutter viel und faktisch der hat faktisch man hat steht faktisch dass dass dass ihn halt halt ihn ihn neuen neuen seinem seinem aus aus seinem seinem seinem seinem seinem seinem vom dem dem in in in des des seinem des negativen auf negativen du du du du ab du du du der der der auch auch Menschen Menschen dann dazu Person auch Menschen Menschen weil eine zu sie sind sind sie die sie sie die trotzdem zu zu zu zu zu zu zu zu zu zu zu genauso Leute genauso zu zu Berichterstattung von zu Berichterstattung und und und bei und und dem dem dem und auf und und gleich und und auch noch noch Erfahrungen Erfahrungen wird noch noch bei noch noch noch \n",
            "Der sie das es ist von die der sich sich sich aber lässt sich sich aber dieser mit Wenn mit mit man man Sauna Sauna Anteil ist ist wegen als oder sich sich ob \n",
            "94 dass die viel ich ich ich kann ich ich kann ich kann kann ich ich \n",
            "Cool das die als als extrem die die die die rechtlichen die die \n",
            "Erdnussspass sein dein dein zu zu ist \n",
            "kontra gesagt dir ihrer Partnerin stimme stimme stimme total interessiert zugehörig total den seiner allem seiner seiner zu Ich Ich zu weil und Wenn Wenn und und und und und schon einfach es einmal einmal es handelst es es es wäre wäre wäre dann dann halt dann dann \n",
            "Demnächst Blumen & mich mich Gedankengang als ich ich Geschlechterrollen nicht bin bin ich ich einem was weiter \n",
            "und auf Geschlecht nicht Der \n",
            "Und Und die die die ist lässt halt die ist die die die ist ist \n",
            "kontra der dran Perspektive mit mit mit nachdem ich ich ich nicht nicht nicht nicht durch ziemlich von Hormonbehandlungen lange lange lange lange lange wie weshalb was leider gesagt was zum zum und zum der und sich sich Am sich sich sich der und Wenn und ganz Angst Angst und ich ich ich ich sehr neuen es dem dem dem kann Namen mir mir mir mir neuen heute dem dem nur nur gut Diskussion der der der der ähnlich von Diskussion Ende Ende sein dass wohl wohl auch nicht auch auch dann ich ich Diskussion so ich so Ich was was was was nur auf nur nur jetzt bei jetzt so so paar paar andere du Charakter die negativen negativen negativen Studie jedoch gehen Gründe Gründe von Psychotherapeuten muss muss aber aber oder zu aber aber gehen vor Und ich man man ist man man den man Und Und auch auch auch auch Und auch auch und ich wird auch auch auch gut auch Und viele zu zu vor vor vor gesehen aber vor zu idf gesehen es es ich gesehen zu zu Person Person zu WIRKLICH und und ist zu zu Detransition und sollte vor sollte Person sollte sollte Person Person so Person Person \n",
            "die die sich sich nicht ist nicht nicht nicht ist ist und \n",
            "kontra solltet von aller aller gesund Es gibt gibt gibt falschen und Juristin Fall Personen Personen sagen es Nachfahren echt echt und zum zum zu und wir wenn wenn zu zu zu zu diesen und und fast fast vielen dass der meiner meiner sehr des des prekären Gutachten auch Gruppe auch die auch auch die die die die die sich sich von des So vor von meisten und heute und und wird diesen diesen auch und und Verwirrung Schritt auch und Schritt und und und und und das paar paar paar das das sich paar paar negativen negativen vor negativen erst vertretbar vertretbar vertretbar allerdings allerdings allerdings von zu von von auch ist auch auch auch von der sind hier Weg der der Weg Weg Weg eine eine eine \n",
            "Hey die die hat hat zu zu zu ist ist ist \n",
            "Überall die sein sich sich es sein zu zu ist kaum \n",
            "Größe der von von gibt gibt gibt der ich Reaktion Reaktion ich man nicht nicht nicht ganze zu allem allem und und und mein mein mein und mir was für und was was was sich sich sich sehr sehr ich sehr sehr sehr einfach es es und und und es es es es und es und und es und und und und und halt halt halt gibt die die die \n",
            "Größe Dank hat Formaten eigentlich mit mit dem in in es es es wie er dran Hat Hat Hat Hat wir kann Hormonbehandlungen bin extrem extrem bin haben haben auch aber schon wir was die die die die sich viel auch auch sich bleibt neuen neuen nach nach gibt gibt Mutter gibt auf das das denke das das das und mir dann sich sich steht sich sich sich sich sich steht es es auch auch wenn eben Dinge auch auch hinter sie dass weg machen faktisch ihre ihre endlich endlich endlich endlich - vom vom wie Rest Thema nur vom wenn einmal Diskussion einer Thema komplett komplett komplett komplett komplett der komplett komplett und nach der der dem dem dem dem eben den eher nach nach den nicht oder den oder oder sind sind sind nicht Leute sind sind tun auch als als auch auch auch schon von von von von von schon schon von der von eben der der der der von deswegen von man von muss man man nicht nicht \n",
            "Godfather kontroversen kontroversen Videos von die die Jahren eine eine eine wurde wurde ist als Das Ich die die die durch durch durch Dazu ob und und durch \n",
            "Der Ich die die die die die für für für für für ist niemand aber einfach aber aber aber für ist ist ist ist \n",
            "und zu \n",
            "Muss hier hier sich nicht ihrer ein zu viel viel zu ist eine es gut gut gut \n",
            "Themenvorschlag ich ich ich ich ich ich \n",
            "Der Der Der Der Der Der Der Größe ja das „mal ich sein sein Thema Thema für Gerade Vor allem welche ich Ich die genug total als ich sich Frauenquote ich sie ich das ich oder Angst Angst \n",
            "Der Ich die „mal lieber hier Thema Thema ich ich mich sehr ich sehr der dem soll Hier Hier länger länger länger hier hier gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich gründlich Und Und \n",
            "die die die sich ganz ganz sich sich sich sich sich sich sich sich sich gut von gut und \n",
            "David Bak Bak Tochter von heute heute heute atmest muss wirklich ist zu zu ist ich sowas tatsächlich tatsächlich tatsächlich tatsächlich tatsächlich ist zu zu \n",
            "kontra Dank der bei hat das das Transmänner Lia ich gleich nicht und bei bei bei sei so wenn wenn Frau wenn wenn wenn wenn Frau Frau groß Ich Frau als als als als als als machen es machen sich eigentlich sich neuen sich durch neuen sich sich sich sich neuen neuen mit sich sich sich sich neuen sich Mutter Mutter Mutter bescheuerte Übergangsphase ihm ihm jeweilige jeweilige ihn Geschlecht 2 andere Ende andere sie sie andere auf auf auf auf auch auf auf auf auf hinaus Laut Grundgesetz Rechte Schule Rechte Rechte Rechte + religiösen uns religiösen religiösen benachteiligt genauso bevorzugt eine eine und und und und und Gleiches eine eine eine eine und und und und und und und \n",
            "94 ich ich ich ich kann ich ich kann halt halt mehr \n",
            "Größe der der die von Hormonbehandlungen der der muss es nicht gleich gleich gleich Theorien Theorien nicht wie ist ist ist halt viele halt geht gut gut und es es es es es werden durch werden werden nie nie nie vulnerable ich auch auch aber aber der ist damit ist ist und diese damit und sich sich ausgesehen viel viel nicht dass ich eine eine \n",
            "Der Der dattel Kanal von sein sein sein ich sich ich ich bin bin bin nicht ist ist ist ist \n",
            "Erdnussspass nicht ihre ihre die du ihre pronomen nicht benutzt \n",
            "haha einfach Es ich ich ich ich ich ich ich ich ich ich nicht ich Ich Ich bin bin mir mir es es es es was was scheiß ist ist ist \n",
            "Demnächst Barth die die ja von zu im im sowas nicht viel \n",
            "kontra Dank gesagt hab hab gesund Es denn denn denn total total total latente sich im dem geprägt er er Denn das das das Geschlecht sich sich es es es es es es es es es es es es es es es zu es es es es nicht es zu zu \n",
            "San gesagt ist ist ist den den die die sehr sehr sehr ich sehr ich allen bin ich sehr die die die die die aber ist ist ist ist ist ist ist ist ist Gründe ist Gründe Gründe Gründe Gründe und und und \n",
            "Diese Diese konnte konnte ja Wenn und dem dem Ich stimme die falschen dass und und 2 2 2 plötzlich Trans mir es mir mir mir sehr es es es es er wie schon schon schon schon was was was was was was was was zu was zu der es es sehr sehr sehr sehr Frau sehr Frau Frau sehr Frau verzichtet verzichtet verzichtet verzichtet verzichtet verzichtet verzichtet Transferleistungen in in in uns uns verzichtet uns uns uns uns uns Begriffen Begriffen seiner seiner diese und zu zu ist uns uns zu im im zu im uns uns im im ein und und und und und und des und und und und und und und und biologischen und paar und ab von wird oder in oder oder oder zu Frau Frau zu oder oder keine oder oder oder und zu Frau und und und und und ihn Transmännern zwischen in eine dem und und in und coping schwierig schwierig und und Definition und weg Definition sind sind von von von von von ich es es Entscheidung ein ein Daher von von in den viel nicht Momentan in in in in von in in meisten von von in in in den in einen in sofort sofort in sofort Erfahrungen Erfahrungen aller ihre dazu sollte sollte sollte nicht an ein ein an der an in in deswegen halt nicht nicht nicht nicht nicht beste auf nicht nicht nicht nicht auszutreten nicht nicht Kombination nicht genauso Kombination nicht nicht nicht nicht nicht nicht nicht nicht nicht werden eine eine eine und in in in in den den in in in in sollte sollte simpel auch auch bei werden auch auch bei bei man werden man werden man man man \n",
            "Der ich es es einfach so \n",
            "Muss Kanal und und ich Thema Thema Thema ich ich ich ich mit ich völliger Überzeugung sehr ist ist gut es Respekt Meister \n",
            "San Andrés es es schon Verein dem von die sehr sehr muss aber zu ist ist ist ist ist ist ist ist ist aber ist ist ist ist ist ist ist und und \n",
            "Schränkt Ich es es sich für dir ich ich zu ich im \n",
            "kontra Wenn weit weit dann nicht nicht Frau an an der der Geschlechtseintrag eintragen eine eine eine eine eine ich sehr sehr sehr sehr sehr sehr sehr bin sehr und Ich bin gesagt oder gesagt gesagt Es Es einfach einfach einfach einfach einfach ich ich wieder wieder und dafür dafür und und ich Ich immer ich ich ich das das neuen ich ich das und und und und hätte hätte hätte will mir mir mir mir mir mir mir mir ich tatsächlich mir mir mir eben eben eben ich so so so viele genau viele Trans die die die man Erkrankungen auf auf auf auf auf auf ich ich auf versuchen versuchen auf auf auf ganzes ganzes ganzes auf auf Ende sozialen Am Am auf die von auch auch eben trotzdem der auch eben das Geschlechtsidentität eines eines Menschen eines die die eine zu zu Menschen zu zu zu zu zu zu zu keine den den den den den zu zu sind was sind was sind sind sind sogar nicht vor vor nicht nicht zu zu zu zu Berichterstattung Laufenden Laufenden Sendung Laufenden Laufenden Laufenden Sendung Sendung Laufenden auszutreten auszutreten auszutreten Sendung und und Luzerner Luzerner Wie Laufenden und und und und und Menschenverstand und und und andere und und und und und und andere und und und erhalten und Aufmerksamkeit wärend andere andere ignoriert ignoriert sagt auch man auch noch noch \n",
            "kontra kontra Dank gesagt weit es es es ist gibt gibt die eintragen zu ist ist ist ist ist ein am für als kein ja ja ja der auch auch auch Das Das ist ist es es es es ist ist Es ist die die die \n",
            "Muss Muss Kanal hier mit mit mit mit Thema zu ist ist zu zu zu in Energieversorgung der der zeit zeit seine Video den andere andere andere Körper \n",
            "Größe gesagt und ihrer Es hört dem heutzutage total total es es sehr sehr sehr sehr sehr sehr eine und und und Ich Ich eher es vermittelt für es aber aber für ist für für sehr einer einer und sehr die die die ähnlich Leuten anders eigene und und gibt und dann ähnlich ähnlich auch auch \n",
            "Größe Seite verstehen verstehen gleichzeitig schon für des ich gesehen Genau mit den zu ziemlich es bestimmen wissenschaftlich sich sich sich sich sich viele sich viele sich und die und eigene eigene und Angst durch klar Angst des der der der sich der der sich sich sich sich sich oder ganzes ganzes als oder oder nur halt auch wissen wissen halt auf andere andere auf auf \n",
            "konstruktiv ja lieber lieber ist völlig beim Fehlverhalten Sobald aber aber aber ist geprägt mal selber selber ist am Ich Ich sowas dass dass dass dass dass dass sich sehr sehr sehr sehr ist ist viel dass dass ist größte Teil ist ist es es ist ist \n",
            "Das zu dein dein zu zu \n",
            "Der Ich total als dir 🤗 Die heutzutage Gesellschaft ist total darauf alles alles zu meist meist oft oft zu sehr oft sehr zu einen Bei sehr Bedenken einen sehr ist sehr ist ist ist ist ist Trotzdem sich einen Dame die einfach die die die sie sie sie und früher früher die auf auf auf auf immer weil immer sie immer immer eigentlich vermutlich warum vermutlich in machen machen die die nie nie die die die über über wirklich als andere \n",
            "Größe Größe Dank als als es Dafür und von sowieso in im immer lässt sich sein sein wir Gesetze die kann die bin die die niemand werden die sich sich man ist ihm ihm \n",
            "San gesagt ist ist ist den den die die sehr sehr sehr ich sehr ich allen bin ich sehr die die die die die aber ist ist ist ist ist ist ist ist ist Gründe ist Gründe Gründe Gründe Gründe und und und \n",
            "Trotzdem die die sich sich sich zu sich ist ist ist das vor vor es es es \n",
            "San was was was von gibt Thema interessiert interessiert interessiert interessiert zu zu ist ist ist ist ist mal anders anders halt ertragen wie als können können die viel es es es und unter unter \n",
            "kontra Wenn Wenn werden werden dann Es aber Es Recht nicht nicht eintragen Ich es es aus wenn wenn und Ich und und und und der und und und potentiell und und oder oder und und oder für mit ist für für sich für sich sich Trotzdem ich ich ich meiner dass dass neuen neuen neuen nach einfach neuen es immer immer immer immer immer die die die die wenn die von dem dem dem dem von halt vom eigentlich du auf auf auf sozialen von auf Ende Ende auf auf auf auf Schritt faktisch faktisch faktisch Problemen Problemen 2 dass 2 versuchen versuchen wirklich auf sozialen Kommunikation Kommunikation mich mich auf der falschen und und und der falschen falschen interessante Schritt gerade eben gerade gerade gerade eben auch auch eben eine eine die eine auch eine die die die die die die die die Person ist und und ebenso einer und und und und ist und auf auf den und ich und auch auch auch auch auch auch auch auch oder auch Und Und und Entscheidung und und und und und und und und ich gut ich und auch auch auch und So weg ich ich ich wäre wäre wäre habe habe habe habe habe habe man habe habe habe man man muss dass man sehr nicht nicht Frage nicht nicht Frage Frage und und und und nicht nicht muss vor vor vor unfruchtbar nicht unfruchtbar deswegen deswegen deswegen nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht nicht schnell schnell schnell nicht schnell und und muss muss schnell schnell schnell muss war ein von eine eine auch auch auch \n",
            "Themenvorschlag Themenvorschlag Verbot die sein sein ich nicht nicht nicht \n",
            "Er gesagt gibt von gibt gibt ich Thema muss ist Vor Vor eine eine die ist ist ist tatsächlich tatsächlich er ist aber aber aber ist ist was ist ist \n",
            "Größe bin einfach Es der der Sinn Sinn Thema total den man Wenn man man ist der sein oder ja ja es machen zu es zu zu es es will sich das das \n",
            "Kanal Kanal hier mit ihre mit mit Boomern zu Kommentare sich sich verwundern wurde Zahlen dort bin sind sind ist ist ist \n",
            "Der Der es es es es es es weiter weiter weiter \n",
            "San gesagt ich ein ein dauerhaft den die war eben die die scheint mit dir dir nicht du dir so wollte ist ist zeit ist ist gegen ist ist absprechen absprechen ihm steht steht \n",
            "Der Größe Größe Dank einfach einfach heute heute heute extrem Genau Genau sich dann dann im sich im zu ist denn dem Alter einfach da ist man man man es es Mit zu zu zu \n",
            "schlachter ist ich ich \n",
            "Herr Kanal einfach geboren geboren denn denn in sich sich sich sich eine Wenn in in aber aber ein ein ein einem einem \n",
            "die die ist die die nicht \n",
            "und nicht es es es \n",
            "Größe Dank von eigentlich nach ich dem dem dem stimme ich man ich man allem Handlung Handlung man es genau gut dem dem der sich Bei Wenn und und Penis und Bei für für für in in für immer immer auch vielen vielen vielen vielen Angst klar klar klar Angst klar klar der der der auch auch der Rücken das das kann das kann sich sich dessen Leuten immer sich und und und sich größte dem dem dem Punkt Punkt Punkt Punkt Punkt Punkt Punkt von Keinen Keinen von Keinen Keinen Fragen es es es daraus faktisch wird wird wird wird wird wird es bei wird bei Rapid und und ihre seine seine faktisch ganzes es Person als Person Person über über über über von über von von von von von von von von von zu der der oder vor vor oder vor denn eines eine biologischen einer eine einer einer eine eine eine eine eine eine einer zwischen in und und und mit werde werde andere gibt deren deren diesen - - schon nicht nicht nicht sie sie nicht nicht auch von wie sie sie ein ein ein solche ein als solche ich ich ich sehr die die Laufenden werden dann dann dann dann deswegen deswegen deswegen dann halt halt auch auch auch Personen Personen auch auch auch auch auch auch von zu Laufenden auszutreten auszutreten zu von von zu zu Laufenden Laufenden zu zu zu zu zu MFG MFG Merkmale von vor sich von von sich von eine eine nicht sein auch auch auch noch noch Außerdem noch von von von von von aller Mühe man man man man man man man man man man man man man man man man man man man man man man man man man man man sollte sollte sollte sollte man man Frage nicht nicht nicht was nicht die die an an und und Außerdem und und haben der haben für für haben haben sein noch an an sein an an man der der seinem sich mit mit unfruchtbar unfruchtbar mit unfruchtbar eben Aussage eben Aussage Aussage dass nicht nicht nicht nicht nicht Laufenden nicht Kombination Kombination nicht Laufenden nicht nicht eine muss gegenteiligen muss und Geschlecht und und zu unfruchtbar zu Definition sofort zu zu zu zu zu zu zu zu eine eine eine zu eine mit der Kombination Kombination der Kombination Kombination Kombination aller deren in nicht nicht \n",
            "Größe \" willst und und zwingt ich ja ja auf auf auf dann auf ich auf deren du du du du du du du du du du nicht werden werden Personen Personen ich werden niemand auf auf auf auf was was was was was was was was was dass einfach einfach nicht einfach nicht nicht einfach einfach \n",
            "bis es die die die medizinisch sich auf auf auf auf gleich eingestuft halt halt einfach Es Es Ich mal mal mal legal hab hab sein es es es es sehr mir mir \n",
            "Größe Geschichte Judith von von dem dem Partnerin hört hört hört Thema total belasse belasse ich nicht nicht nicht nicht nicht genug ist sehr ist sehr sehr für die Bei sehr einfach sie für für für ist und und ist ist für hätte ich ich ich ich und und und Operationen viel viel viel die faktisch der der der sich die sich sich sich sich sich sich sich sich sich neuen neuen Am Am Verwirrung einer gehört dass dass es bei den von von unserer von von von den den \n",
            "Klassische will ja ja das so so MEINE MEINE dir dir Physisch Physisch weiter \n",
            "gesagt gesagt und und sein Thema Aber warum Aber Aber soll Geschlecht Aber ist aber ist ist minderjährige minderjährige ist ist ist ist ist ist ist \n",
            "Hey ich es viel kann kann ich ich ich ich ich \n",
            "kontra umfassend umfassend kleine beim eben und und und und ich ich ich denken kann kann so ziemlich bin mit mit mit mit mit mit und bin bin bin und und schon für mit mit sehr Frauen und für und und und ich ich ich ich ich ein ein ich ich ich warum nur sie ich ich ich ich auch auch eben auch ich ich auch eben auch eben eben auch auch auch gut gut gut gut eben gut so so so Am Ende auf auf auf auf auf auf auf so eben eben eben nicht eben Am eben nicht eben eben muss dem dem dem dem dem dem dem dem dem der der auf Körperregion der der selbstzerstörendes zwischen zwischen \n",
            "Der nicht die die du halt zu zu wie sowas sowas \n",
            "Der Der und Dank gesagt gesagt lieber lieber deine als belasse interessiert ich ich ich wirklich wirklich ich wirklich nett zu du du du sowas ja ja tatsächlich tatsächlich die die die die die die die die lieber lieber \n",
            "San gesagt jeder jeder gibt gibt Das in die den den den den den zu gemacht und die Meinung sich sich sich niemand sich ist ist ist ist ist ist einen man der der Gründe und der einer einer \n",
            "San gesagt oft sich sind schon schon schon schon ein froh froh das Wenn das kann tatsächlich Ende das das das das kann kann auch auch sollte sollte \n",
            "Barth Also von realitätsgetreu so sich Hormonbehandlungen Hormonbehandlungen bin dir sowas sowas ist du du ist \n",
            "Größe ich jeder jeder von Es der der der interessiert total haben hier Vor eine es es das das Ich zugehörig queer das das die für für sich tatsächlich sich und die die die bspw man man nie den den der der der der der der der der viel mit frei das das Warum Problem Problem der entscheiden entscheiden \n",
            "Der die für ich einfach einfach einfach zu einfach einfach ist \n",
            "Er in es es von Transsexualtät ich betrachtet Wenn deren Namensänderung Namensänderung gibt Geschlechtsangleichungen die die die sich sich sich sich aber man einer einer faktisch einer einer zu \n",
            "Der Größe Größe Dank einfach einfach heute heute heute extrem Genau Genau sich dann dann im sich im zu ist denn dem Alter einfach da ist man man man es es Mit zu zu zu \n",
            "Ist es die Menschen gibt gibt Thema ich ich sehr sehr einfach einfach einfach einfach einfach einfach ist einfach einfach einfach einfach einfach wie was einfach wie haha haha \n",
            "Er gesagt gesagt Es Es denn die total ist total ist ist ist ist ist ist ist ist ist ein eher Forderung zwar zwar sehr sehr sehr für ist ist ist ist ist ist ist \n",
            "Themenvorschlag dass sein sein ja ich ich ich ich ich ich ich ich gut \n",
            "Der Ich bei bei konnte richtige mehr oft oft leichter interessiert im ist ist betont seiner seiner Und genug genug genug genug die die die die die von die die die die die zeit zeit zeit der Angst des des halt des halt halt halt halt halt halt steht jemals andere \n",
            "Größe der der solltet lieber ist ist Euch Euch wirklich muss Gerade Gerade dies auch auch auch allem ist auch Vorgang Das Hormonbehandlungen Hormonbehandlungen nämlich sich sich vielseitiger gestalten sich sich sich sich sich und und und und und dem dem dem Gründe und dem dem dem dem und dem Mutter meine Angst steht steht leider sich sich leider Aber halt wie wie wie wie sich halt wie wie wie wie Intolerante man man wie und ist Trans wie ist ist ist ist ist ist ist ist \n",
            "Er gesagt es es sich die die Muss Muss Das total total total andere Raum ist gerne Fälle Fälle kein einfach einfach ist ist ist ist \n",
            "dattel Kanal sein ich ich ich ich ich ich ich nicht ist nicht ist ist ist \n",
            "Größe Größe bin als Es ich ich ich ich ich ich ich ich ich ich dass den den bin ich in kaum kaum was was was was man ist man man man man man man man es einem einfach einfach einfach \n",
            "San gesagt es ihrer ihrer denn denn von dem in sich wurde sich du einfach einfach sowas sich einfach einfach einfach ist einfach ist ist \n",
            "Erdnussspass ich ich Gutachter ich ich unterstützen weiter \n",
            "Größe gesagt und und und Dafür ich der interessiert der deren den deren meistens Hormonbehandlungen Hormonbehandlungen meistens die die in sich wenn wenn sich sich sich sich sich sich mit mit mit mit mit sich mit mit sich der sich sich sich steht steht steht steht steht dann dann \n",
            "Größe gesagt und und und Dafür ich der interessiert der deren den deren meistens Hormonbehandlungen Hormonbehandlungen meistens die die in sich wenn wenn sich sich sich sich sich sich mit mit mit mit mit sich mit mit sich der sich sich sich steht steht steht steht steht dann dann \n",
            "Größe Seite verstehen verstehen gleichzeitig schon für des ich gesehen Genau mit den zu ziemlich es bestimmen wissenschaftlich sich sich sich sich sich viele sich viele sich und die und eigene eigene und Angst durch klar Angst des der der der sich der der sich sich sich sich sich oder ganzes ganzes oder oder oder nur halt auch wissen wissen halt auf andere andere auf auf \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      index                                      encoder_input  \\\n",
              "991     156  Trotzdem sollte man davor die Psyche von denen...   \n",
              "126       3    \" ja schön .  Diese Person kann sich ja auch...   \n",
              "847     740    ,  sie sind ein Mensch der Kultur und sprech...   \n",
              "1472     31   \" Ihr klaut den Patienten wichtige Buchstaben...   \n",
              "1874    525  Ich ging die ganze Zeit davon aus ,  dass dies...   \n",
              "1636     13                              Gerne ,  bitte  :  )    \n",
              "704     346  Die neue Melanie  -  Endlich Leben interessant...   \n",
              "1312    158   \" Statistisch gesehen sollte es in jeder Klas...   \n",
              "1444    193  35 : 40 Wenn man keinen Diskurs mit der Gegens...   \n",
              "599      40  94 daß ist mir durchaus bewusst ,  aber die Au...   \n",
              "\n",
              "                                                   pred  \\\n",
              "991   Trotzdem die Psyche von denen lässt aber sich ...   \n",
              "126   Der Der kontra Einfach finde jeder Weil für de...   \n",
              "847   Godfather die dir ganz sprechen ganz Viele bea...   \n",
              "1472                           Der die die die die die    \n",
              "1874  kontra der dran Perspektive mit mit mit nachde...   \n",
              "1636                                       Der ich ich    \n",
              "704   Größe Seite total Weil bitte In Thema In Thema...   \n",
              "1312  Der Ich und und chirurgisch und es Wenn der de...   \n",
              "1444  Der sie das es ist von die der sich sich sich ...   \n",
              "599   94 dass die viel ich ich ich kann ich ich kann...   \n",
              "\n",
              "                                         correct_output  \n",
              "991   Trotzdem sollte man davor die Psyche von denen...  \n",
              "126    \"ja schön. Diese Person kann sich ja auch dem...  \n",
              "847   Sie sind ein Mensch der Kultur und sprechen in...  \n",
              "1472  \"Ihr klaut den Patienten wichtige Buchstaben.\"...  \n",
              "1874  Ich ging die ganze Zeit davon aus, dass dieses...  \n",
              "1636                                    Gerne, bitte :)  \n",
              "704   Die neue Melanie - Endlich Leben interessanter...  \n",
              "1312  \"Statistisch gesehen sollte es in jeder Klasse...  \n",
              "1444  35:40 Wenn man keinen Diskurs mit der Gegensei...  \n",
              "599   94 dass ist mir durchaus bewusst, aber die Aus...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-406e85f3-e0e8-4c89-b087-d8ad871166d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>encoder_input</th>\n",
              "      <th>pred</th>\n",
              "      <th>correct_output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>156</td>\n",
              "      <td>Trotzdem sollte man davor die Psyche von denen...</td>\n",
              "      <td>Trotzdem die Psyche von denen lässt aber sich ...</td>\n",
              "      <td>Trotzdem sollte man davor die Psyche von denen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>3</td>\n",
              "      <td>\" ja schön .  Diese Person kann sich ja auch...</td>\n",
              "      <td>Der Der kontra Einfach finde jeder Weil für de...</td>\n",
              "      <td>\"ja schön. Diese Person kann sich ja auch dem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>740</td>\n",
              "      <td>,  sie sind ein Mensch der Kultur und sprech...</td>\n",
              "      <td>Godfather die dir ganz sprechen ganz Viele bea...</td>\n",
              "      <td>Sie sind ein Mensch der Kultur und sprechen in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>31</td>\n",
              "      <td>\" Ihr klaut den Patienten wichtige Buchstaben...</td>\n",
              "      <td>Der die die die die die</td>\n",
              "      <td>\"Ihr klaut den Patienten wichtige Buchstaben.\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1874</th>\n",
              "      <td>525</td>\n",
              "      <td>Ich ging die ganze Zeit davon aus ,  dass dies...</td>\n",
              "      <td>kontra der dran Perspektive mit mit mit nachde...</td>\n",
              "      <td>Ich ging die ganze Zeit davon aus, dass dieses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1636</th>\n",
              "      <td>13</td>\n",
              "      <td>Gerne ,  bitte  :  )</td>\n",
              "      <td>Der ich ich</td>\n",
              "      <td>Gerne, bitte :)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704</th>\n",
              "      <td>346</td>\n",
              "      <td>Die neue Melanie  -  Endlich Leben interessant...</td>\n",
              "      <td>Größe Seite total Weil bitte In Thema In Thema...</td>\n",
              "      <td>Die neue Melanie - Endlich Leben interessanter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>158</td>\n",
              "      <td>\" Statistisch gesehen sollte es in jeder Klas...</td>\n",
              "      <td>Der Ich und und chirurgisch und es Wenn der de...</td>\n",
              "      <td>\"Statistisch gesehen sollte es in jeder Klasse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>193</td>\n",
              "      <td>35 : 40 Wenn man keinen Diskurs mit der Gegens...</td>\n",
              "      <td>Der sie das es ist von die der sich sich sich ...</td>\n",
              "      <td>35:40 Wenn man keinen Diskurs mit der Gegensei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>40</td>\n",
              "      <td>94 daß ist mir durchaus bewusst ,  aber die Au...</td>\n",
              "      <td>94 dass die viel ich ich ich kann ich ich kann...</td>\n",
              "      <td>94 dass ist mir durchaus bewusst, aber die Aus...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-406e85f3-e0e8-4c89-b087-d8ad871166d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-406e85f3-e0e8-4c89-b087-d8ad871166d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-406e85f3-e0e8-4c89-b087-d8ad871166d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "sample_train = train.sample(100)\n",
        "sample_train['pred'] = sample_train.apply(lambda row: inference(str(row['encoder_input']), str(row['decoder_output'])), axis=1)\n",
        "\n",
        "sample_train['correct_output'] = sample_train['decoder_output']\n",
        "\n",
        "sample_train.drop(['decoder_input', 'decoder_output'], axis=1, inplace=True)\n",
        "\n",
        "sample_train.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Additional evaluation***"
      ],
      "metadata": {
        "id": "73FqrI8LUjLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEyVbvuVz31b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db780e4-6279-43e2-90d0-e418a66b320b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def precision(candidate, reference, n):\n",
        "    \"\"\"\n",
        "    Calculate the precision of n-grams in a text.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "    - n (int): The n-gram order\n",
        "\n",
        "    Returns:\n",
        "    - float: The precision score\n",
        "    \"\"\"\n",
        "    candidate_ngrams = ngrams(word_tokenize(candidate), n)\n",
        "    reference_ngrams = ngrams(word_tokenize(reference), n)\n",
        "    candidate_ngrams_set = set(candidate_ngrams)\n",
        "    reference_ngrams_set = set(reference_ngrams)\n",
        "    common_ngrams = candidate_ngrams_set.intersection(reference_ngrams_set)\n",
        "    precision = len(common_ngrams) / len(candidate_ngrams_set)\n",
        "    return precision"
      ],
      "metadata": {
        "id": "L2Sib5t7SjZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train['precision'] = sample_train.apply(lambda row: brevity_penalty(row['pred'], row['correct_output'], 2) if len(row['correct_output']) > 0 else None, axis=1)\n",
        "\n",
        "mean_precision = sample_train['precision'].mean(skipna=True)\n",
        "print(mean_precision)\n"
      ],
      "metadata": {
        "id": "i9t_36BLTufD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def brevity_penalty(candidate, reference):\n",
        "    \"\"\"\n",
        "    Calculate the brevity penalty for the precision score.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "\n",
        "    Returns:\n",
        "    - float: The brevity penalty\n",
        "    \"\"\"\n",
        "    candidate_length = len(word_tokenize(candidate))\n",
        "    reference_length = len(word_tokenize(reference))\n",
        "    if candidate_length > reference_length:\n",
        "        brevity_penalty = 1\n",
        "    else:\n",
        "        brevity_penalty = np.exp(1 - reference_length / candidate_length)\n",
        "    return brevity_penalty"
      ],
      "metadata": {
        "id": "gTCB2jMCSloG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train['brevity_penalty'] = sample_train.apply(lambda row: brevity_penalty(row['pred'], row['correct_output'], 2) if len(row['correct_output']) > 0 else None, axis=1)\n",
        "\n",
        "mean_brevity_penalty = sample_train['brevity_penalty'].mean(skipna=True)\n",
        "print(mean_brevity_penalty)\n"
      ],
      "metadata": {
        "id": "9oTvg3l-TlZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gleu(candidate, reference, max_order=4):\n",
        "    \"\"\"\n",
        "    Calculate the GLEU score for a generated text compared to a reference text.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "    - max_order (int): The maximum n-gram order to consider (default: 4)\n",
        "\n",
        "    Returns:\n",
        "    - float: The GLEU score\n",
        "    \"\"\"\n",
        "    precision_scores = []\n",
        "    for n in range(1, max_order + 1):\n",
        "        precision_scores.append(precision(candidate, reference, n))\n",
        "    brevity_penalty_score = brevity_penalty(candidate, reference)\n",
        "    gleu_score = brevity_penalty_score * np.exp(np.mean(np.log(precision_scores)))\n",
        "    return gleu_score"
      ],
      "metadata": {
        "id": "Eb6pLI0mSn0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_train['gleu'] = sample_train.apply(lambda row: gleu(row['pred'], row['correct_output'], 2) if len(row['correct_output']) > 0 else None, axis=1)\n",
        "\n",
        "mean_gleu = sample_train['gleu'].mean(skipna=True)\n",
        "print(mean_gleu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z49J09MgSqOF",
        "outputId": "b7285d4c-bb88-4b70-9e24-dd0c28f1f198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-4694358a9c7a>:17: RuntimeWarning: divide by zero encountered in log\n",
            "  gleu_score = brevity_penalty_score * np.exp(np.mean(np.log(precision_scores)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03317760027044629\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOy+EQl7Mb4NJs/WWnOnSR",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}