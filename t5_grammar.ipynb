{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LidiiaMelnyk95/FSU_Jena_scripts/blob/main/t5_grammar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwi4rBTXJLIo"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgIIF_dzJ-ye",
        "outputId": "a72b92b4-855f-420c-d37f-8e4110adc62b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LB0z7aV_KEMN"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LzJlkGRKGOy"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('/content/data_augmented_df.csv', sep = ';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCAa0FXwKept"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns = {\"SPELLING\": 'input', 'Comment': 'output'})\n",
        "df = df[['input', 'output']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFECAqQzRHLH",
        "outputId": "bb048c4e-cdfc-45d4-d591-8e8689810790"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0epWsr7KOcG"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration, T5Tokenizer,\n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "  )\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv5looCmKRJB"
      },
      "outputs": [],
      "source": [
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klPJsD-jKTXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calc_token_len(example):\n",
        "    return len(tokenizer(example).input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBr6YT_sKXCm"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.05, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5FkTt5bKZJB",
        "outputId": "3b7bb071-1f5c-4777-96ec-ab61718f5c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoNf3PEBKlnV"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu2HsOqSKphu"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GrammarDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer,print_text=False):\n",
        "        self.dataset = dataset\n",
        "        self.pad_to_max_length = False\n",
        "        self.tokenizer = tokenizer\n",
        "        self.print_text = print_text\n",
        "        self.max_len = 64\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "    def tokenize_data(self, example):\n",
        "        input_, target_ = example['input'], example['output']\n",
        "\n",
        "        # tokenize inputs\n",
        "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length,\n",
        "                                            max_length=self.max_len,\n",
        "                                            return_attention_mask=True)\n",
        "\n",
        "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length,\n",
        "                                            max_length=self.max_len,\n",
        "                                            return_attention_mask=True)\n",
        "\n",
        "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
        "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
        "            \"labels\": tokenized_targets['input_ids']\n",
        "        }\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        inputs = self.tokenize_data(self.dataset[index])\n",
        "\n",
        "        if self.print_text:\n",
        "            for k in inputs.keys():\n",
        "                print(k, len(inputs[k]))\n",
        "\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA2SG_YVLK-F",
        "outputId": "b149a5c6-04bd-405c-9937-7c66a23332a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids 64\n",
            "attention_mask 64\n",
            "labels 64\n",
            "{'input_ids': [3, 7422, 632, 4209, 8775, 632, 308, 8491, 680, 3, 15, 7, 78, 548, 13832, 745, 6, 211, 3, 362, 15638, 17955, 5335, 218, 3494, 1662, 20899, 425, 36, 18992, 35, 6368, 6, 1352, 615, 211, 236, 5964, 31499, 5, 196, 51, 3, 30240, 2010, 3, 362, 96, 19629, 2626, 4039, 157, 13513, 121, 19642, 561, 177, 10122, 29, 7937, 501, 2149, 649, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3, 7422, 632, 4209, 8775, 632, 308, 8491, 680, 3, 15, 7, 78, 548, 13832, 745, 6, 211, 3, 362, 15638, 17955, 5335, 218, 3494, 1662, 20899, 425, 36, 18992, 35, 6368, 6, 1352, 615, 211, 236, 5964, 31499, 5, 196, 51, 3, 30240, 2010, 3, 362, 96, 19629, 2626, 4039, 157, 13513, 121, 19642, 561, 177, 10122, 29, 7937, 501, 2149, 649, 1]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = GrammarDataset(test_dataset, tokenizer, True)\n",
        "print(dataset[15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFZE-pHVLPBN"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install rouge_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DNbV3EILQ9K"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_metric\n",
        "rouge_metric = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAZzgn9aLT3b"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDUK_2TaLWcb"
      },
      "outputs": [],
      "source": [
        "# defining training related arguments\n",
        "batch_size = 5\n",
        "args = {\n",
        "    \"output_dir\": \"/content/drive/MyDrive/c4_200m/weights\",\n",
        "    \"evaluation_strategy\": \"steps\",\n",
        "    \"per_device_train_batch_size\": batch_size,\n",
        "    \"per_device_eval_batch_size\": batch_size,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"save_total_limit\": 2,\n",
        "    \"predict_with_generate\": True,\n",
        "    \"gradient_accumulation_steps\": 6,\n",
        "    \"eval_steps\": 5,\n",
        "    \"save_steps\": 5,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"logging_dir\": \"/logs\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL5Z68N8LnE1",
        "outputId": "25fe8424-97a9-40c9-e57f-8fc4dec2057d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh2975MBLpyR"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(model=model,\n",
        "                args=args,\n",
        "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
        "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
        "                tokenizer=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdiHNdl7LsUe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PydWPTpThKW"
      },
      "outputs": [],
      "source": [
        "trainer.predict(test_dataset= test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51FdH8LIR7JT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "def correct_grammar(input_text, num_return_sequences):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(torch_device)\n",
        "    translated = model.generate(input_ids,\n",
        "                                max_length=512,\n",
        "                                num_beams=4,\n",
        "                                num_return_sequences=num_return_sequences,\n",
        "                                temperature=1.5)\n",
        "\n",
        "\n",
        "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    return tgt_text\n",
        "\n",
        "# Example usage\n",
        "input_text = test_df['input'][70]\n",
        "tgt_text = correct_grammar(input_text, num_return_sequences = 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
        "\n",
        "# Define an example input shape\n",
        "input_shape = (1, 512)  # Replace with your desired input shape\n",
        "\n",
        "# Use torchsummary to visualize the model architecture\n",
        "summary(model, 512)\n"
      ],
      "metadata": {
        "id": "H1X8NLTB8v5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPDUr9AHR92Y"
      },
      "outputs": [],
      "source": [
        "test_df['preds'] = test_df['input'].apply(lambda x: correct_grammar(x, num_return_sequences=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "5GfUz8Kw4QJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchsummary\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to CPU\n",
        "model.to('cpu')\n",
        "\n",
        "# Get the input shape of the model\n",
        "input_text = test_df['input'][70]\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "input_shape = tuple(input_ids.shape[1:])\n",
        "\n",
        "# Use torchsummary to visualize the model architecture\n",
        "torchsummary.summary(model, input_size=input_shape, device='cpu')\n"
      ],
      "metadata": {
        "id": "k3R1ujzN3dBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "TlsgrcG34eHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Create a dummy input\n",
        "input_text = test_df['input'][70]\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(input_ids)\n",
        "\n",
        "# Create a graph of the model\n",
        "graph = make_dot(outputs)\n",
        "\n",
        "# Save the graph as a PDF\n",
        "graph.render(\"model_graph\")\n"
      ],
      "metadata": {
        "id": "LfNrtO8p6z3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDpLoS7dXBuj"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('compared_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "vfGSh8yj5Y1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def precision(candidate, reference, n):\n",
        "    \"\"\"\n",
        "    Calculate the precision of n-grams in a text.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "    - n (int): The n-gram order\n",
        "\n",
        "    Returns:\n",
        "    - float: The precision score\n",
        "    \"\"\"\n",
        "    candidate_ngrams = ngrams(word_tokenize(candidate), n)\n",
        "    reference_ngrams = ngrams(word_tokenize(reference), n)\n",
        "    candidate_ngrams_set = set(candidate_ngrams)\n",
        "    reference_ngrams_set = set(reference_ngrams)\n",
        "    common_ngrams = candidate_ngrams_set.intersection(reference_ngrams_set)\n",
        "    precision = len(common_ngrams) / len(candidate_ngrams_set)\n",
        "    return precision"
      ],
      "metadata": {
        "id": "DegFZHjL5d6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def brevity_penalty(candidate, reference):\n",
        "    \"\"\"\n",
        "    Calculate the brevity penalty for the precision score.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "\n",
        "    Returns:\n",
        "    - float: The brevity penalty\n",
        "    \"\"\"\n",
        "    candidate_length = len(word_tokenize(candidate))\n",
        "    reference_length = len(word_tokenize(reference))\n",
        "    if candidate_length > reference_length:\n",
        "        brevity_penalty = 1\n",
        "    else:\n",
        "        brevity_penalty = np.exp(1 - reference_length / candidate_length)\n",
        "    return brevity_penalty"
      ],
      "metadata": {
        "id": "eYU0gZ_I5gUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gleu(candidate, reference, max_order=4):\n",
        "    \"\"\"\n",
        "    Calculate the GLEU score for a generated text compared to a reference text.\n",
        "\n",
        "    Args:\n",
        "    - candidate (str): The generated text\n",
        "    - reference (str): The reference text\n",
        "    - max_order (int): The maximum n-gram order to consider (default: 4)\n",
        "\n",
        "    Returns:\n",
        "    - float: The GLEU score\n",
        "    \"\"\"\n",
        "    precision_scores = []\n",
        "    for n in range(1, max_order + 1):\n",
        "        precision_scores.append(precision(candidate, reference, n))\n",
        "    brevity_penalty_score = brevity_penalty(candidate, reference)\n",
        "    gleu_score = brevity_penalty_score * np.exp(np.mean(np.log(precision_scores)))\n",
        "    return gleu_score"
      ],
      "metadata": {
        "id": "vLdgeVyo5i1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['preds'] = test_df['preds'].apply(lambda x: x[0])\n"
      ],
      "metadata": {
        "id": "sjYZ-qZP5lbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/compared_df.csv')"
      ],
      "metadata": {
        "id": "h9_Kh_a76cZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isna().sum()"
      ],
      "metadata": {
        "id": "aXtMJAvD6uqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in test_df.iterrows():\n",
        "    try:\n",
        "        test_df.at[i, 'gleu'] = gleu(row['preds'], row['output'], max_order=20)\n",
        "    except ZeroDivisionError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "nao1B69k5zve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gleu_score = test_df.gleu.mean()"
      ],
      "metadata": {
        "id": "e7xRF0Xh6h4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gleu_score"
      ],
      "metadata": {
        "id": "m8BV3rLQ7M8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('')"
      ],
      "metadata": {
        "id": "wcNL0RYncLmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YurCqYuY-xl"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# Install the happytransformer library\n",
        "!pip install happytransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEIa4gN-UJON"
      },
      "outputs": [],
      "source": [
        "from happytransformer import HappyTextToText, TTSettings\n",
        "happy_tt = HappyTextToText(\"T5\", \"google/flan-t5-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv4SFiZkZJk4"
      },
      "outputs": [],
      "source": [
        "args = TTSettings(num_beams=5, min_length=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFl5VxDTZLVL"
      },
      "outputs": [],
      "source": [
        "test_df['preds_2'] = test_df['input'].apply(lambda x:  happy_tt.generate_text(\"grammar: {}\".format(x), args=args).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIaeZuVhZaR5"
      },
      "outputs": [],
      "source": [
        "test_df.to_csv('t5_large_corrected.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in test_df.iterrows():\n",
        "    try:\n",
        "        test_df.at[i, 'gleu_2'] = gleu(row['preds_2'], row['output'], max_order=20)\n",
        "    except ZeroDivisionError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "eXs4OGNM8_7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['gleu_2'].mean()"
      ],
      "metadata": {
        "id": "mIM8gVhn9KP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n"
      ],
      "metadata": {
        "id": "1M0_M_m49WAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in test_df.iterrows():\n",
        "    try:\n",
        "        test_df.at[i, 'bleu'] = sentence_bleu([row['preds']], row['output'])\n",
        "        test_df.at[i, 'bleu_2'] = sentence_bleu([row['preds_2']], row['output'])\n",
        "    except ZeroDivisionError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "MnivH4U49cUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.bleu.mean()"
      ],
      "metadata": {
        "id": "vfIgv0xk9pZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.bleu_2.mean()"
      ],
      "metadata": {
        "id": "nR1-wxgM9rwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n"
      ],
      "metadata": {
        "id": "T9aWtvxf-gi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matching_sentences = sum([1 for ref, corr in zip(test_df['output'], test_df['preds_2']) if ref == corr])\n",
        "precision = matching_sentences / len(test_df['preds'].values)"
      ],
      "metadata": {
        "id": "syI8k944-ygy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "from Levenshtein import distance"
      ],
      "metadata": {
        "id": "Q4riRpXR_QwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in test_df.iterrows():\n",
        "    try:\n",
        "        test_df.at[i, 'distance'] = distance([row['preds']], row['output'])\n",
        "        test_df.at[i, 'distance_2'] = distance([row['preds_2']], row['output'])\n",
        "    except ZeroDivisionError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "Wb0kyCKiADoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.distance.mean()"
      ],
      "metadata": {
        "id": "HAphUncgAKzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.distance_2.mean()"
      ],
      "metadata": {
        "id": "5uGhQSi5AOyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPFMZ72sYoOd"
      },
      "source": [
        "Basically here we tried to aaply the t5 model, which was trained on the huge dataset and is applied through finding the grammatically correct translation.\n",
        "It seems like the model performs even worse than language tool itself.\n",
        "therefore, will try to implement this\n",
        "https://towardsdatascience.com/deep-text-corrector-using-monotonic-attention-with-dataset-creation-1e1a3f5a1b9e"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQUH+nr13BsVvMnF4gUx5/",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}